{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from node2vec import Node2Vec\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians\n",
    "    # lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 236258 entries, 0 to 236257\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   lng     236258 non-null  float64\n",
      " 1   lat     236258 non-null  float64\n",
      " 2   id      236258 non-null  int64  \n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 5.4 MB\n"
     ]
    }
   ],
   "source": [
    "nodes_df = pd.read_csv('data/road_intersection_nodes.csv')\n",
    "nodes_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.291857</td>\n",
       "      <td>0.710457</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.291856</td>\n",
       "      <td>0.710460</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.291854</td>\n",
       "      <td>0.710463</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.291852</td>\n",
       "      <td>0.710466</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.291850</td>\n",
       "      <td>0.710468</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236253</th>\n",
       "      <td>-1.289822</td>\n",
       "      <td>0.709714</td>\n",
       "      <td>2414372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236254</th>\n",
       "      <td>-1.290694</td>\n",
       "      <td>0.711094</td>\n",
       "      <td>2414375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236255</th>\n",
       "      <td>-1.290695</td>\n",
       "      <td>0.711093</td>\n",
       "      <td>2414376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236256</th>\n",
       "      <td>-1.290695</td>\n",
       "      <td>0.711091</td>\n",
       "      <td>2414377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236257</th>\n",
       "      <td>-1.290696</td>\n",
       "      <td>0.711090</td>\n",
       "      <td>2414378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236258 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             lng       lat       id\n",
       "0      -1.291857  0.710457        0\n",
       "1      -1.291856  0.710460        1\n",
       "2      -1.291854  0.710463        2\n",
       "3      -1.291852  0.710466        3\n",
       "4      -1.291850  0.710468        4\n",
       "...          ...       ...      ...\n",
       "236253 -1.289822  0.709714  2414372\n",
       "236254 -1.290694  0.711094  2414375\n",
       "236255 -1.290695  0.711093  2414376\n",
       "236256 -1.290695  0.711091  2414377\n",
       "236257 -1.290696  0.711090  2414378\n",
       "\n",
       "[236258 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_df[['lng', 'lat']] = nodes_df[['lng', 'lat']].apply(np.vectorize(radians))\n",
    "nodes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "lat_center, lng_center = nodes_df.lat.mean(), nodes_df.lng.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11704, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "G.graph['Name'] = 'TLC'\n",
    "\n",
    "RADIUS = 4\n",
    "node_to_idx = {}\n",
    "\n",
    "for idx, (_, row) in enumerate(nodes_df.iterrows()):\n",
    "    if haversine(lng_center, lat_center, row.lng, row.lat) > RADIUS:\n",
    "       continue\n",
    "    G.add_node(int(row.id), id=int(row.id) ,lng=row.lng, lat=row.lat)\n",
    "    node_to_idx[int(row.id)] = idx\n",
    "\n",
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "observed_nodes = list(G.nodes())\n",
    "obs_nodes_set = set(observed_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>id</th>\n",
       "      <th>pickups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5902</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19416</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5903</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19417</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5904</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19418</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5905</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19420</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19422</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35910369</th>\n",
       "      <td>152.0</td>\n",
       "      <td>401970</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35910370</th>\n",
       "      <td>152.0</td>\n",
       "      <td>401973</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35910371</th>\n",
       "      <td>152.0</td>\n",
       "      <td>401974</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35910491</th>\n",
       "      <td>152.0</td>\n",
       "      <td>872198</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35910811</th>\n",
       "      <td>152.0</td>\n",
       "      <td>1206243</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1779008 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            day       id  pickups\n",
       "5902        1.0    19416      3.0\n",
       "5903        1.0    19417      2.0\n",
       "5904        1.0    19418      2.0\n",
       "5905        1.0    19420      2.0\n",
       "5906        1.0    19422      4.0\n",
       "...         ...      ...      ...\n",
       "35910369  152.0   401970      0.0\n",
       "35910370  152.0   401973      0.0\n",
       "35910371  152.0   401974      0.0\n",
       "35910491  152.0   872198      0.0\n",
       "35910811  152.0  1206243      0.0\n",
       "\n",
       "[1779008 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickups_df = pd.read_csv('data/TLC_daily.csv')\n",
    "pickups_df = pickups_df[pickups_df['id'].isin(obs_nodes_set)]\n",
    "pickups_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 282983 entries, 0 to 282982\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   olng    282983 non-null  float64\n",
      " 1   olat    282983 non-null  float64\n",
      " 2   dlng    282983 non-null  float64\n",
      " 3   dlat    282983 non-null  float64\n",
      " 4   oid     282983 non-null  int64  \n",
      " 5   did     282983 non-null  int64  \n",
      "dtypes: float64(4), int64(2)\n",
      "memory usage: 13.0 MB\n"
     ]
    }
   ],
   "source": [
    "edges_df = pd.read_csv('data/road_intersection_edges.csv')\n",
    "edges_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11704, 15021)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _, row in edges_df.iterrows():\n",
    "    if row.oid not in obs_nodes_set or row.did not in obs_nodes_set:\n",
    "        continue\n",
    "\n",
    "    dist = haversine(row.olng, row.olat, row.dlng, row.dlat)\n",
    "\n",
    "    G.add_edge(int(row.oid), int(row.did), weight=dist)\n",
    "    \n",
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11704/11704 [00:00<00:00, 23325.06it/s]\n",
      "Generating walks (CPU: 1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.19s/it]\n",
      "Generating walks (CPU: 3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.24s/it]\n",
      "Generating walks (CPU: 2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.22s/it]\n",
      "Generating walks (CPU: 4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "embedder = Node2Vec(G, dimensions=128, walk_length=30, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "model = embedder.fit(window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19416, 19417)\n",
      "(19416, 195466)\n",
      "(19416, 195451)\n",
      "(19416, 388769)\n",
      "(19417, 19418)\n"
     ]
    }
   ],
   "source": [
    "for idx, e in enumerate(G.edges()):\n",
    "    if idx == 5:\n",
    "        break\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# model.wv.most_similar('19459')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# embedding = nn.Embedding(G.number_of_nodes(), 128)\n",
    "# embedding.weight.requires_grad = False\n",
    "\n",
    "for idx, node in enumerate(G.nodes()):\n",
    "    G.add_node(int(node), x=model.wv[str(node)].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11704"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class DayObservationsDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        super().__init__()\n",
    "        self.data = df['id'].to_numpy()\n",
    "        self.targets = df['pickups'].to_numpy()\n",
    "        self.observed_nodes = set(np.unique(self.data))\n",
    "\n",
    "        self.node_to_target = df.set_index('id')['pickups'].to_dict()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # returns node, and its observation\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "    def get_observation_by_node(self, node):\n",
    "        return self.node_to_target[node]\n",
    "\n",
    "    def get_observed_nodes(self):\n",
    "        return self.observed_nodes\n",
    "\n",
    "\n",
    "def get_dataset_by_day(pickups_df, day):\n",
    "    df = pickups_df[pickups_df['day'].astype('int') == day].copy()\n",
    "    df.drop('day', axis=1, inplace=True)\n",
    "    return DayObservationsDataset(df)\n",
    "\n",
    "\n",
    "ds = get_dataset_by_day(pickups_df, 12)\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5734 2458 3512\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
    "\n",
    "np.random.seed(228)\n",
    "\n",
    "indices = list(range(len(ds)))\n",
    "\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.3)\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=0.3)\n",
    "print(len(train_indices), len(val_indices), len(test_indices))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(ds, batch_size=128, sampler=train_sampler)\n",
    "val_loader = DataLoader(ds, batch_size=len(val_sampler.indices), sampler=val_sampler)\n",
    "test_loader = DataLoader(ds, batch_size=len(test_sampler.indices), sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[11704, 128], edge_index=[2, 30042], id=[11704], lng=[11704], lat=[11704], weight=[30042])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "pyg_graph = from_networkx(G)\n",
    "pyg_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000e+00, 7.0563e-02, 1.0056e+00, 4.1295e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 3.9692e-02, 1.4919e+01, 1.3713e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 3.7000e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         9.8177e-02, 0.0000e+00], dtype=torch.float64, grad_fn=<SliceBackward0>),\n",
       " tensor([ 0.,  0.,  2.,  0.,  0.,  0.,  0.,  0., 13.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.], dtype=torch.float64))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def weight_fn(dists, lamb):\n",
    "    return torch.exp(-lamb * dists)\n",
    "\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, pyg_graph, nodes_df, observs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.g = pyg_graph\n",
    "        self.nodes_df = nodes_df\n",
    "        self.obs_nodes = observs[0]\n",
    "        self.obs_targets = observs[1]\n",
    "        self.node_to_gidx = dict(zip(self.g.id.numpy(), range(len(self.g.id))))\n",
    "\n",
    "        self.neighbors = NearestNeighbors(n_neighbors=15, metric='haversine')\n",
    "        self.obs_nodes_locs = self.nodes_df.iloc[np.vectorize(node_to_idx.get)(self.obs_nodes)]\n",
    "        self.neighbors.fit(self.obs_nodes_locs[['lat', 'lng']].values)\n",
    "\n",
    "        self.k = nn.Parameter(torch.rand(1))\n",
    "        # self.k = torch.tensor([1.0])\n",
    "        self.lambda_1 = nn.Parameter(torch.rand(1))\n",
    "        self.lambda_2 = nn.Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # getting nearest observed nodes\n",
    "        dists, indices = self.neighbors.kneighbors(nodes_df.iloc[np.vectorize(node_to_idx.get)(X)][['lat', 'lng']].values)\n",
    "        dists = dists * 6371 * 1000\n",
    "\n",
    "        # so for now dist in meters\n",
    "        if self.training:\n",
    "            dists, indices = dists[:, 1:], indices[:, 1:]\n",
    "        # get_observations = np.vectorize(self.obs_ds.get_observation_by_node)\n",
    "        # observations = get_observations(self.obs_nodes_locs['id'].values[indices])\n",
    "        observations = self.obs_targets[indices]\n",
    "\n",
    "        dists, observations = torch.as_tensor(dists), torch.as_tensor(observations)\n",
    "\n",
    "        neighbors_indices = np.vectorize(self.node_to_gidx.get)(self.obs_nodes[indices])\n",
    "        neighbors_embeds = self.g.x[neighbors_indices.reshape(-1)].reshape(*neighbors_indices.shape, -1)\n",
    "\n",
    "        X_embeds = self.g.x[np.vectorize(self.node_to_gidx.get)(X)]\n",
    "        similarities = nn.functional.cosine_similarity(X_embeds[:, None], neighbors_embeds, dim=2)\n",
    "        # similarities = torch.sum(X_embeds[:, None] * neighbors_embeds, dim=-1)\n",
    "        # print(similarities)\n",
    "\n",
    "        dist_weights = weight_fn(dists, self.lambda_1)\n",
    "        simi_weights = weight_fn(similarities, self.lambda_2)\n",
    "        # sum normalizization\n",
    "        dist_weights = nn.functional.normalize(dist_weights, p=1)\n",
    "        simi_weights = nn.functional.normalize(simi_weights, p=1)\n",
    "\n",
    "        f = self.k * dist_weights + (1 - self.k) * simi_weights\n",
    "\n",
    "        result = torch.sum(f.mul(observations), dim=-1)\n",
    "\n",
    "        return result\n",
    "\n",
    "predictor = Predictor(pyg_graph, nodes_df, ds[train_indices])\n",
    "kek = next(iter(train_loader))\n",
    "predictor(kek[0])[:20], kek[1][:20] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.4960, Val loss: 0.9012, Val R2: 0.8655\n",
      "Epoch 5, Loss: 1.0314, Val loss: 1.0785, Val R2: 0.8391\n",
      "Epoch 10, Loss: 1.0354, Val loss: 1.0779, Val R2: 0.8392\n",
      "Epoch 15, Loss: 1.0267, Val loss: 1.0794, Val R2: 0.8389\n",
      "Epoch 20, Loss: 1.0253, Val loss: 1.0768, Val R2: 0.8393\n",
      "Epoch 25, Loss: 1.0249, Val loss: 1.0701, Val R2: 0.8403\n",
      "Epoch 30, Loss: 1.0115, Val loss: 1.0663, Val R2: 0.8409\n",
      "Epoch 35, Loss: 1.0155, Val loss: 1.0647, Val R2: 0.8411\n",
      "Epoch 40, Loss: 1.0185, Val loss: 1.0638, Val R2: 0.8413\n",
      "Epoch 45, Loss: 1.0154, Val loss: 1.0635, Val R2: 0.8413\n",
      "Epoch 50, Loss: 1.0086, Val loss: 1.0633, Val R2: 0.8413\n",
      "Epoch 55, Loss: 1.0246, Val loss: 1.0635, Val R2: 0.8413\n",
      "Epoch 60, Loss: 1.0064, Val loss: 1.0629, Val R2: 0.8414\n",
      "Epoch 65, Loss: 1.0140, Val loss: 1.0624, Val R2: 0.8415\n",
      "Epoch 70, Loss: 1.0130, Val loss: 1.0627, Val R2: 0.8414\n",
      "Epoch 75, Loss: 1.0052, Val loss: 1.0625, Val R2: 0.8415\n",
      "Epoch 80, Loss: 1.0198, Val loss: 1.0621, Val R2: 0.8415\n",
      "Epoch 85, Loss: 1.0042, Val loss: 1.0616, Val R2: 0.8416\n",
      "Epoch 90, Loss: 1.0029, Val loss: 1.0615, Val R2: 0.8416\n",
      "Epoch 95, Loss: 1.0057, Val loss: 1.0615, Val R2: 0.8416\n",
      "Epoch 100, Loss: 1.0026, Val loss: 1.0611, Val R2: 0.8417\n",
      "k Parameter containing:\n",
      "tensor([0.8159], requires_grad=True)\n",
      "lambda_1 Parameter containing:\n",
      "tensor([0.1320], requires_grad=True)\n",
      "lambda_2 Parameter containing:\n",
      "tensor([-3.9346], requires_grad=True)\n",
      "(0.7612585389864063, 0.8719819767615833)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "loss_fn = nn.HuberLoss(delta=100)\n",
    "optimizer = torch.optim.Adam(predictor.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.7)\n",
    "\n",
    "\n",
    "def calc_score(pred, actual):\n",
    "    return r2_score(actual, pred)\n",
    "\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    scores = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (X, y) in loader:\n",
    "            out = model(X)\n",
    "            scores.append(calc_score(out, y))\n",
    "            loss = loss_fn(out, y)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader), np.mean(scores)\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, loss_fn, optimizer, scheduler=None, num_epochs=100):\n",
    "    losses = []\n",
    "    test_scores = []\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i_step, (X, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X)\n",
    "            loss = loss_fn(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        total_loss /= len(train_loader)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            val_loss, score = test(model, val_loader)\n",
    "            test_scores.append(score)\n",
    "            print(f'Epoch {epoch}, Loss: {losses[-1]:.4f}, Val loss: {val_loss:.4f}, Val R2: {test_scores[-1]:.4f}')\n",
    "\n",
    "train(predictor, train_loader, val_loader, loss_fn, optimizer, scheduler)\n",
    "for name, param in predictor.named_parameters():\n",
    "    print(name, param)\n",
    "\n",
    "print(test(predictor, test_loader))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30452e0fb0b877c71442cfddf3db9e1b032e1699292a3dd400d9a1b61508e43d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('traffic': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
