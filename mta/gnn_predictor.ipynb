{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import glob\n",
    "\n",
    "from typing import Tuple, Union, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "from hyperopt.pyll.stochastic import sample as ho_sample\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchmetrics import R2Score\n",
    "\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "import torch_scatter\n",
    "\n",
    "from src.utils import *\n",
    "from dataset import *\n",
    "from src.train import train, test\n",
    "from src.dataloaders import make_dataloaders_from_dataset\n",
    "from src.model import BaseEstimator, KnnEstimator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(17)\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.readwrite.read_gpickle('data/network32.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch geometric Data object. For now used only for storing node embedding. \n",
    "# Supposed to be used in the future for obtaining node embeddings.\n",
    "pyg_data = from_networkx(G, group_node_attrs=['embedding'])\n",
    "pyg_data.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea behind the extrapolation layer is considering observed and unobserved nodes as a bipartite graph, where each node belongs to the corresponding part(left -> observed, right -> unobserved). So an edge between two nodes defines that estimating target in unobserved location in the right part will be partly based on the observed node in the left part. Using that concept layer can be developed basing on the Message Passing principle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model specification:\n",
    "$$T(a) = \\sum_{b \\in \\mathcal{N_a} } T(b)w(a, b)\\frac{scale(a)}{scale(b)}, \\textrm{where summation is calculated for the k nearest neighbors.}$$\n",
    "\n",
    "$$ w(i, j)= \\frac{exp(LeakyReLU(a^T[W x_i\\parallel W x_j]))}{\\sum_{k \\in \\mathcal{N_i}} exp(LeakyReLU(a^T[W x_i\\parallel W x_k]))} ;$$\n",
    "\n",
    "$scale(a)$ = mean target of location **a**\n",
    "\n",
    "$x_a$ is an embedding for a node **a**;\n",
    "\n",
    "So for this model $W, \\vec{a}$ parameters are optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLocationScaler(nn.Module):\n",
    "    def __call__(self, X):\n",
    "        return X\n",
    "\n",
    "\n",
    "class ExtrapolationLayer(pyg.nn.MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels=1, hidden_channels=16, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.lin = nn.Linear(self.in_channels, self.hidden_channels)\n",
    "        self.loc_scaler = BasicLocationScaler()\n",
    "        self.a = nn.Parameter(torch.randn(self.hidden_channels * 2))\n",
    "    \n",
    "\n",
    "    def forward(self, obs_emb, obs_scaling_features, obs_targets, q_emb, q_scaling_features, edge_index):\n",
    "        \"\"\"Returns estimated targets for query nodes using scaling features and node embeddings.\n",
    "        This function works utilizing message passing principle. For more information check the following link\n",
    "        https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
    "\n",
    "        Args:\n",
    "            obs_emb (torch.Tensor): embeddings for observed nodes \n",
    "            obs_scaling_features (torch.Tensor): scaling features for observed nodes\n",
    "            obs_targets (torch.Tensor): targets for observed nodes\n",
    "            q_emb (torch.Tensor): embeddings for query nodes \n",
    "            q_scaling_features (torch.Tensor): scaling features for query nodes\n",
    "            edge_index (torch.Tensor): edges for the bipartite graph\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: estimated targets for query nodes\n",
    "        \"\"\"\n",
    "        # print(obs_emb.shape)\n",
    "        # print(q_emb.shape)\n",
    "        # print(obs_scaling_features.shape)\n",
    "        # print(q_scaling_features.shape)\n",
    "        # print(obs_targets.shape)\n",
    "\n",
    "        scale_obs = self.loc_scaler(obs_scaling_features)\n",
    "        scale_q = self.loc_scaler(q_scaling_features)\n",
    "\n",
    "        obs_emb = self.lin(obs_emb)\n",
    "        q_emb = self.lin(q_emb)\n",
    "\n",
    "        q_size = len(q_emb)\n",
    "        obs_size = len(obs_emb)\n",
    "\n",
    "\n",
    "        scaled_targets = (obs_targets / scale_obs).reshape(-1, 1)\n",
    "        # scaled_targets = obs_targets.reshape(-1, 1)\n",
    "        out = self.propagate(edge_index, x=scaled_targets, emb=(obs_emb, q_emb), size=(obs_size, q_size)).reshape(-1)\n",
    "        out = out * scale_q\n",
    "        # out = scale_q\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j, emb_j, emb_i, index):\n",
    "        # print(x_j.shape, emb_j.shape, emb_i.shape, index.shape)\n",
    "        # alpha = torch.sum(emb_j * emb_i, dim=-1)[:, None]\n",
    "        # print(emb_j.shape)\n",
    "        # print(alpha.shape)\n",
    "        # alpha = torch.cosine_similarity(emb_j, emb_i)[:, None]\n",
    "\n",
    "        alpha = torch.hstack((emb_j, emb_i))\n",
    "        alpha = torch.nn.functional.leaky_relu(torch.matmul(alpha, self.a[:, None]), negative_slope=0.2)\n",
    "\n",
    "        att_weights = pyg.utils.softmax(alpha, index)\n",
    "        out = att_weights * x_j\n",
    "        return out\n",
    "    \n",
    "    def aggregate(self, inputs, index):\n",
    "        return torch_scatter.scatter(inputs, index, dim=0, reduce='sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "class GNNEstimator(BaseEstimator):\n",
    "    def __init__(self, pyg_graph: pyg.data.Data, nodes: List[str], targets: np.array, in_channels=32, *args, **kwargs):\n",
    "        super().__init__(pyg_graph, nodes, targets, *args, **kwargs)\n",
    "\n",
    "        self.obs_targets = torch.as_tensor(self.obs_targets).to(device)\n",
    "        self.extrapolation_layer = ExtrapolationLayer(in_channels)    \n",
    "\n",
    "    def forward(self, nodes, edge_index):\n",
    "        \"\"\"Returns estimated targets for the given nodes\n",
    "\n",
    "        Args:\n",
    "            nodes (List[str]): unobserved query nodes\n",
    "            edge_index (torch.Tensor): edge index for bipartite graph\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: estimated targets for query nodes\n",
    "        \"\"\"\n",
    "        # here can be possible node features updating\n",
    "        embeddings = self.g.x\n",
    "        q_index = np.asarray([int(self.node_to_idx(x)) for x in nodes])\n",
    "\n",
    "        obs_scaling_features = self.g.mean_target[self.obs_node_indices]\n",
    "        q_scaling_features = self.g.mean_target[q_index]\n",
    "\n",
    "        obs_emb = embeddings[self.obs_node_indices]\n",
    "        q_emb = embeddings[q_index]\n",
    "\n",
    "        return self.extrapolation_layer(obs_emb, obs_scaling_features, \n",
    "            self.obs_targets, q_emb, q_scaling_features, edge_index)\n",
    "    \n",
    "    def make_test_edge_index(self, query_graph: pyg.data.Data, neighbors_num=10):\n",
    "        \"\"\"Forms edge index for bipartite graph, where left part is observed nodes and the right one is query nodes.\n",
    "        Edges are formed only for k nearest observed nodes(by geo distance).\n",
    "\n",
    "        Args:\n",
    "            query_graph (pyg.data.Data): node features of query graph.\n",
    "            neighbors_num (int, optional): number of nearest neighbors to search. Defaults to 10.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: edge index for the bipartite graph.\n",
    "        \"\"\"\n",
    "        nneighbors = NearestNeighbors(n_neighbors=neighbors_num, metric='haversine')\n",
    "\n",
    "        nneighbors.fit(torch.vstack([self.g.lat[self.obs_node_indices],\n",
    "                                     self.g.lon[self.obs_node_indices]]).T.detach().cpu())\n",
    "\n",
    "        indices = nneighbors.kneighbors(torch.vstack([query_graph.lat, query_graph.lon]).T.detach().cpu(), \n",
    "                return_distance=False)\n",
    "        \n",
    "        index = torch.stack((torch.asarray(indices).reshape(-1), \n",
    "                             torch.arange(query_graph.num_nodes)[:, None].expand(-1, neighbors_num).reshape(-1)))\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import Random\n",
    "\n",
    "def node_indices_split(nodes_num, train_size=0.7, val_size=0.15, test_size=0.15):\n",
    "    assert train_size + val_size + test_size == 1.0\n",
    "    indices = np.arange(nodes_num)\n",
    "    Random(34).shuffle(indices)\n",
    "    train, val, test = map(np.sort, np.split(indices, [int(nodes_num*train_size), int(nodes_num*(train_size + val_size))]))\n",
    "    train, val, test = map(torch.tensor, (train, val, test))\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = R2Score().to(device)\n",
    "\n",
    "def test(model, nodes, targets, edge_index, loss_fn, device) -> Tuple[float, float]:\n",
    "    \"\"\" returns average loss and score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(nodes, edge_index)\n",
    "\n",
    "        y_gpu = targets.to(device)\n",
    "        loss = loss_fn(out, y_gpu)\n",
    "    \n",
    "    return loss.item(), R2(y_gpu, out).item()\n",
    "\n",
    "\n",
    "def train(model, train_nodes, y_train, train_edge_index, val_nodes, y_val, val_edge_index, loss_fn,\n",
    "          optimizer, device, scheduler=None, num_epochs=10, plot=True, plot_update_every=5):\n",
    "    \"\"\" returns best model on validation\n",
    "    \"\"\"\n",
    "\n",
    "    train_losses = []\n",
    "    train_scores = []\n",
    "    val_losses = []\n",
    "    val_scores = []\n",
    "\n",
    "    for epoch in range(num_epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_nodes, train_edge_index)\n",
    "        y_train_gpu = y_train.to(device)\n",
    "        loss = loss_fn(out, y_train_gpu)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(y_train, out)\n",
    "        # fuck()\n",
    "   \n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_scores.append(R2(out, y_train_gpu).item())\n",
    "        val_loss, val_score = test(model, val_nodes, y_val, val_edge_index, loss_fn, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_scores.append(val_score)\n",
    "\n",
    "        if plot and epoch > 0 and epoch % plot_update_every == 0:\n",
    "            clear_output(True)\n",
    "            _, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "            \n",
    "            sns.lineplot(ax=axes[0], x=range(epoch + 1), y=train_losses, label='Train', color='blue')\n",
    "            sns.lineplot(ax=axes[0], x=range(epoch + 1), y=val_losses, label='Val', color='red')\n",
    "            axes[0].set_xlabel('Epoch')\n",
    "            axes[0].set_ylabel('Loss')\n",
    "            axes[0].legend()\n",
    "\n",
    "            sns.lineplot(ax=axes[1], x=range(epoch + 1), y=val_scores, label='Val', color='red')\n",
    "            sns.lineplot(ax=axes[1], x=range(epoch + 1), y=train_scores, label='Train', color='blue')\n",
    "            axes[1].set_xlabel('Epoch')\n",
    "            axes[1].set_ylabel('Score')\n",
    "            axes[1].legend()\n",
    "\n",
    "            plt.show()\n",
    "            # print(f'Epoch {epoch}, Loss: {train_losses[-1]:.4f}, Val loss: {val_loss:.4f}, Val R2: {val_scores[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bc2641a30a490b8fdde97ac89a82b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-26, Test loss: 6282.7241,\tTest score: 0.8114\n",
      "2020-12-27, Test loss: 8750.8008,\tTest score: 0.7440\n",
      "2020-12-28, Test loss: 10874.7559,\tTest score: 0.9058\n",
      "2020-12-29, Test loss: 43914.0234,\tTest score: 0.6630\n",
      "2020-12-30, Test loss: 14398.5195,\tTest score: 0.8635\n",
      "2020-12-31, Test loss: 12822.3184,\tTest score: 0.8124\n",
      "2021-01-01, Test loss: 6393.6743,\tTest score: 0.7407\n",
      "2021-01-02, Test loss: nan,\tTest score: nan\n",
      "2021-01-03, Test loss: 16426.0430,\tTest score: 0.2726\n",
      "2021-01-04, Test loss: 15532.9375,\tTest score: 0.8610\n",
      "2021-01-05, Test loss: 46183.9297,\tTest score: 0.6242\n",
      "2021-01-06, Test loss: 55545.0234,\tTest score: 0.5809\n",
      "2021-01-07, Test loss: 46814.9062,\tTest score: 0.5982\n",
      "2021-01-08, Test loss: 19168.2207,\tTest score: 0.7908\n",
      "2021-01-09, Test loss: 19818.6895,\tTest score: 0.5187\n",
      "2021-01-10, Test loss: 6693.1782,\tTest score: 0.7253\n",
      "2021-01-11, Test loss: 10884.4219,\tTest score: 0.8941\n",
      "2021-01-12, Test loss: 5815.1704,\tTest score: 0.9425\n",
      "2021-01-13, Test loss: 8075.1855,\tTest score: 0.9165\n",
      "2021-01-14, Test loss: 11344.7197,\tTest score: 0.9282\n",
      "2021-01-15, Test loss: 20601.1992,\tTest score: 0.8879\n",
      "2021-01-16, Test loss: 9805.6426,\tTest score: 0.8220\n",
      "2021-01-17, Test loss: 5361.0444,\tTest score: 0.8256\n",
      "2021-01-18, Test loss: 4503.0161,\tTest score: 0.9373\n",
      "2021-01-19, Test loss: 15218.4629,\tTest score: 0.8844\n",
      "2021-01-20, Test loss: 14824.0684,\tTest score: 0.8610\n",
      "2021-01-21, Test loss: 8055.6616,\tTest score: 0.9396\n",
      "2021-01-22, Test loss: 5658.2764,\tTest score: 0.9428\n",
      "2021-01-23, Test loss: 5875.6089,\tTest score: 0.8847\n",
      "2021-01-24, Test loss: 5648.7886,\tTest score: 0.7961\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "loss_fn = nn.MSELoss().to(device)\n",
    "\n",
    "model = None\n",
    "NEIGHBORS_NUM = 25\n",
    "\n",
    "for path in tqdm(sorted(glob.glob('datasets/*'))):\n",
    "    day = path.split('/')[1].split('.')[0]\n",
    "\n",
    "    try:\n",
    "        ds = torch.load(path)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # print(f'Dataset size: {len(ds)}')\n",
    "    train_nodes_idx, val_nodes_idx, test_nodes_idx = node_indices_split(len(ds))\n",
    "    train_nodes, train_targets = ds[train_nodes_idx]\n",
    "    val_nodes, val_targets = ds[val_nodes_idx]\n",
    "    test_nodes, test_targets = ds[test_nodes_idx]\n",
    "    y_train, y_val, y_test = map(torch.as_tensor, (train_targets, val_targets, test_targets)) \n",
    "\n",
    "    model = GNNEstimator(pyg_data, train_nodes, train_targets).to(device)\n",
    "    train_graph = pyg_data.subgraph(torch.tensor(model.node_to_idx(train_nodes)))\n",
    "    val_graph = pyg_data.subgraph(torch.tensor(model.node_to_idx(val_nodes)))\n",
    "    test_graph = pyg_data.subgraph(torch.tensor(model.node_to_idx(test_nodes)))\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=1)\n",
    "\n",
    "    # train_edge_index = pyg.nn.knn_graph(train_graph.x, k=NEIGHBORS_NUM, cosine=True) \n",
    "    train_edge_index = model.make_test_edge_index(train_graph, neighbors_num=NEIGHBORS_NUM + 1).to(device)\n",
    "    train_edge_index, _ = pyg.utils.remove_self_loops(train_edge_index)\n",
    "    val_edge_index = model.make_test_edge_index(val_graph, neighbors_num=NEIGHBORS_NUM).to(device)\n",
    "    test_edge_index = model.make_test_edge_index(test_graph, neighbors_num=NEIGHBORS_NUM).to(device)\n",
    "    \n",
    "\n",
    "    train(model, train_nodes, y_train, train_edge_index, val_nodes, y_val, val_edge_index, \n",
    "          loss_fn, optimizer, device, num_epochs=200, plot=False, plot_update_every=30)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 20, gamma=0.9)\n",
    "\n",
    "    test_loss, test_score = test(model, test_nodes, y_test, test_edge_index, loss_fn, device)\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "        # print(name, param)\n",
    "\n",
    "    results[f'{day}'] = test_score \n",
    "    print(f'{day}, Test loss: {test_loss:.4f},\\tTest score: {test_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30452e0fb0b877c71442cfddf3db9e1b032e1699292a3dd400d9a1b61508e43d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('traffic')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
