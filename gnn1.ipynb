{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from typing import Tuple, Union, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# import node2vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "from utils import *\n",
    "from dataset import *\n",
    "from train import train, test, device\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.readwrite.read_gpickle('data/network.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eldervald/miniconda3/envs/traffic/lib/python3.9/site-packages/torch_geometric/utils/convert.py:170: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  data[key] = torch.tensor(value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[60789, 128], edge_index=[2, 151294], lng=[60789], lat=[60789], id=[60789], dist=[151294])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch geometric Data object. For now used only for storing node embedding. \n",
    "# Supposed to be used in the future for obtaining node embeddings.\n",
    "pyg_graph = from_networkx(G)\n",
    "pyg_graph.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$T(a) = \\sum_{b \\neq a,\\space b\\subset Train } T(b)w(a, b), \\textrm{where summation is calculated for the {\\bf k} nearest neighbors.}$$\n",
    "\n",
    "$$ w(a,b)= \\frac{e^{u(a,b)}}{\\sum_{b \\neq a} e^{u(a, b)}} ;$$\n",
    "\n",
    "$u(a, b) = f(x_a, x_b, dist_{a,b}) = MLP(concat[hadamard(x_a, x_b), dist_{a, b}]);$\n",
    "\n",
    "$hadamard(x_a, x_b)$ - per coordinate product;\n",
    "\n",
    "$x_a$ is an embedding for a node **a**;\n",
    "\n",
    "So for this model $MLP$ parameters are optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(64, 1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.layers(X)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Estimator(nn.Module):\n",
    "    def __init__(self, pyg_graph: pyg.data.Data, observations: Tuple[List, List]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.g = pyg_graph\n",
    "        self.obs_nodes = observations[0]\n",
    "        self.obs_targets = observations[1]\n",
    "\n",
    "        self.mlp = MLP(128 + 1)\n",
    "\n",
    "        self.NEIGHBORS_NUM = 15\n",
    "        \n",
    "        # dicts for fast indexing\n",
    "        self.node_to_gidx = np.vectorize(dict(zip(self.g.id.detach().cpu().numpy(), range(len(self.g.id)))).get)\n",
    "        \n",
    "        self.neighbors = NearestNeighbors(n_neighbors=self.NEIGHBORS_NUM, metric='haversine')\n",
    "        obs_nodes_indices = self.node_to_gidx(self.obs_nodes)\n",
    "        self.neighbors.fit(torch.vstack([self.g.lat[obs_nodes_indices], self.g.lng[obs_nodes_indices]]).T.detach().cpu())\n",
    "\n",
    "        self.k = nn.Parameter(torch.rand(1))\n",
    "        # self.k = torch.tensor([1.0]).to(device)\n",
    "        self.lambda_1 = nn.Parameter(torch.rand(1))\n",
    "        self.lambda_2 = nn.Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # getting nearest observed nodes\n",
    "        X_indices = self.node_to_gidx(X.detach().cpu())\n",
    "        dists, indices = self.neighbors.kneighbors(torch.vstack([self.g.lat[X_indices], self.g.lng[X_indices]]).T.detach().cpu())\n",
    "        # converting dists to km\n",
    "        dists = dists * 6371\n",
    "\n",
    "        # skipping loc by itself\n",
    "        if self.training:\n",
    "            dists, indices = dists[:, 1:], indices[:, 1:]\n",
    "        \n",
    "        observations = self.obs_targets[indices.reshape(-1)].reshape(*indices.shape)\n",
    "\n",
    "        dists, observations = torch.as_tensor(dists).to(device), torch.as_tensor(observations).to(device)\n",
    "\n",
    "        # finding corresponding node embedding of neighbors\n",
    "        neighbors_indices = self.node_to_gidx(self.obs_nodes[indices.reshape(-1)])\n",
    "        neighbors_embeds = self.g.x[neighbors_indices].reshape(*indices.shape, -1)\n",
    "\n",
    "        # computing similarities between node ans its neighbors\n",
    "        X_embeds = self.g.x[X_indices]\n",
    "\n",
    "        input = torch.cat([torch.mul(X_embeds[:, None].expand(-1, neighbors_embeds.shape[1], -1),\n",
    "                                     neighbors_embeds),\n",
    "                           dists[:, :, None]\n",
    "                           ], dim=2)\n",
    "\n",
    "        \n",
    "        out = self.mlp(input.reshape(-1, input.shape[-1]).float()).reshape(-1, neighbors_embeds.shape[1])\n",
    "\n",
    "        # normalizization\n",
    "        # att_weights = nn.functional.normalize(out, p=1)\n",
    "        att_weights = nn.functional.softmax(out, dim=-1)\n",
    "\n",
    "        # interpolation\n",
    "        result = torch.sum(att_weights.mul(observations), dim=-1)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d001bc65d84181b9bbc794511f21d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1, Test loss: 41.6902, test score: 0.9322\n",
      "Day 2, Test loss: 43.1477, test score: 0.8884\n",
      "Day 3, Test loss: 36.8675, test score: 0.9031\n",
      "Day 4, Test loss: 38.9109, test score: 0.8825\n",
      "Day 5, Test loss: 44.6251, test score: 0.9180\n",
      "Day 6, Test loss: 43.7092, test score: 0.8600\n",
      "Day 7, Test loss: 42.5549, test score: 0.9178\n",
      "Day 8, Test loss: 46.4163, test score: 0.9267\n",
      "Day 9, Test loss: 57.7076, test score: 0.9236\n",
      "Day 10, Test loss: 49.4633, test score: 0.9149\n",
      "Day 11, Test loss: 42.1883, test score: 0.9280\n",
      "Day 12, Test loss: 49.6712, test score: 0.9011\n",
      "Day 13, Test loss: 43.9908, test score: 0.9571\n",
      "Day 14, Test loss: 47.2227, test score: 0.9414\n",
      "Day 15, Test loss: 42.9286, test score: 0.9393\n",
      "Day 16, Test loss: 57.7090, test score: 0.9298\n",
      "Day 17, Test loss: 57.1782, test score: 0.9279\n",
      "Day 18, Test loss: 38.4185, test score: 0.9398\n",
      "Day 19, Test loss: 43.2382, test score: 0.9364\n",
      "Day 20, Test loss: 44.6762, test score: 0.8789\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.HuberLoss(delta=20).to(device)\n",
    "\n",
    "for day in tqdm(range(1, 21)):\n",
    "    path = f'datasets/day_{day}.dat'\n",
    "\n",
    "    try:\n",
    "        ds = torch.load(path)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    train_loader, val_loader, test_loader = make_data_loaders_from_dataset(ds, train_batch_size=64)\n",
    "    train_batches = [batch for batch in train_loader]\n",
    "    train_nodes = torch.cat([batch[0] for batch in train_batches])\n",
    "    train_targets = torch.cat([batch[1] for batch in train_batches])\n",
    "\n",
    "    model = Estimator(pyg_graph, (train_nodes, train_targets)).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 20, gamma=0.9)\n",
    "\n",
    "    best_model = train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs=30, plotting=False)\n",
    "    test_loss, test_score = test(best_model, test_loader, loss_fn)\n",
    "\n",
    "    # for name, param in best_model.named_parameters():\n",
    "        # print(name, param)\n",
    "\n",
    "    results[f'day_{day}'] = test_score \n",
    "    print(f'Day {day}, Test loss: {test_loss:.4f}, test score: {test_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('results/gnn1.json', mode='w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30452e0fb0b877c71442cfddf3db9e1b032e1699292a3dd400d9a1b61508e43d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('traffic': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
